# Review of publicly available implementations
Summary of existing github repo, articles, Kaggle kernels that achieve interesting performances (excluding ensembling approached for now)



## No Deep Learning

Implementations that do not involve deep learning techniques (classic computer vision approaches such as descriptors, etc.)

- Name of the resource ([link](https://www.google.com/)): *short explanation of core implementation ideas*



## Deep Learning without convolution

Implementations that only involve non-convolutional layers

- Name of the resource ([link](https://www.google.com/)): *short explanation of core implementation ideas*



## CNN without pretrained models

Implementations that involve CNN but no pretrained models

- Name of the resource ([link](https://www.google.com/)): *short explanation of core implementation ideas*



## Customization of pretrained models

Implementations that involve common Imagenet pretrained models

- ResNeXt ([link](https://www.kaggle.com/stalkermustang/pytorch-pretraiedmodels-se-resnext101-baseline/notebook)): *resnext101 is a simple, highly modularized network architecture for image classification. Our network is constructed by repeating a building block that aggregates a set of transformations with the same topology. Our simple design results in a homogeneous, multi-branch architecture that has only a few hyper-parameters to set. This strategy exposes a new dimension, which we call “cardinality” (the size of the set of transformations), as an essential factor in addition to the dimensions of depth and width.*
- VGG16 [link](https://www.kaggle.com/gimunu/training-augmentation-and-pretrained-vgg16-model): kernel from the last challenge which used pretrained VGG16.. no comments or discussion provided though.